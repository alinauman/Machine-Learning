{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "In this notebook we will use the Normal Equation, Gradient Descent and Perceptron Learning. Furthermore, we will check the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data using pandas dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:\\Projects\\MLProjects\\mnist_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59999, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's have a look into the rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       7  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  ...    0.658  0.659  \\\n",
       "0     2  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "1     1  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "2     0  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "3     4  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "4     1  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "5     4  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "6     9  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "7     5  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "8     9  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9     0  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "10    6  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "11    9  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "12    0  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "13    1  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "14    5  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "15    9  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "16    7  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "17    3  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "18    4  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "19    9  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "20    6  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "21    6  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "22    5  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "23    4  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "24    0  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "25    7  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "26    4  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "27    0  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "28    1  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "29    3  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "...  .. ..  ...  ...  ...  ...  ...  ...  ...  ...  ...      ...    ...   \n",
       "9969  5  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9970  2  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9971  4  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9972  9  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9973  4  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9974  3  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9975  6  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9976  4  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9977  1  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9978  7  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9979  2  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9980  6  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9981  5  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9982  0  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9983  1  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9984  2  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9985  3  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9986  4  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9987  5  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9988  6  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9989  7  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9990  8  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9991  9  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9992  0  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9993  1  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9994  2  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9995  3  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9996  4  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9997  5  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "9998  6  0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "\n",
       "      0.660  0.661  0.662  0.663  0.664  0.665  0.666  0.667  \n",
       "0         0      0      0      0      0      0      0      0  \n",
       "1         0      0      0      0      0      0      0      0  \n",
       "2         0      0      0      0      0      0      0      0  \n",
       "3         0      0      0      0      0      0      0      0  \n",
       "4         0      0      0      0      0      0      0      0  \n",
       "5         0      0      0      0      0      0      0      0  \n",
       "6         0      0      0      0      0      0      0      0  \n",
       "7         0      0      0      0      0      0      0      0  \n",
       "8         0      0      0      0      0      0      0      0  \n",
       "9         0      0      0      0      0      0      0      0  \n",
       "10        0      0      0      0      0      0      0      0  \n",
       "11        0      0      0      0      0      0      0      0  \n",
       "12        0      0      0      0      0      0      0      0  \n",
       "13        0      0      0      0      0      0      0      0  \n",
       "14        0      0      0      0      0      0      0      0  \n",
       "15        0      0      0      0      0      0      0      0  \n",
       "16        0      0      0      0      0      0      0      0  \n",
       "17        0      0      0      0      0      0      0      0  \n",
       "18        0      0      0      0      0      0      0      0  \n",
       "19        0      0      0      0      0      0      0      0  \n",
       "20        0      0      0      0      0      0      0      0  \n",
       "21        0      0      0      0      0      0      0      0  \n",
       "22        0      0      0      0      0      0      0      0  \n",
       "23        0      0      0      0      0      0      0      0  \n",
       "24        0      0      0      0      0      0      0      0  \n",
       "25        0      0      0      0      0      0      0      0  \n",
       "26        0      0      0      0      0      0      0      0  \n",
       "27        0      0      0      0      0      0      0      0  \n",
       "28        0      0      0      0      0      0      0      0  \n",
       "29        0      0      0      0      0      0      0      0  \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "9969      0      0      0      0      0      0      0      0  \n",
       "9970      0      0      0      0      0      0      0      0  \n",
       "9971      0      0      0      0      0      0      0      0  \n",
       "9972      0      0      0      0      0      0      0      0  \n",
       "9973      0      0      0      0      0      0      0      0  \n",
       "9974      0      0      0      0      0      0      0      0  \n",
       "9975      0      0      0      0      0      0      0      0  \n",
       "9976      0      0      0      0      0      0      0      0  \n",
       "9977      0      0      0      0      0      0      0      0  \n",
       "9978      0      0      0      0      0      0      0      0  \n",
       "9979      0      0      0      0      0      0      0      0  \n",
       "9980      0      0      0      0      0      0      0      0  \n",
       "9981      0      0      0      0      0      0      0      0  \n",
       "9982      0      0      0      0      0      0      0      0  \n",
       "9983      0      0      0      0      0      0      0      0  \n",
       "9984      0      0      0      0      0      0      0      0  \n",
       "9985      0      0      0      0      0      0      0      0  \n",
       "9986      0      0      0      0      0      0      0      0  \n",
       "9987      0      0      0      0      0      0      0      0  \n",
       "9988      0      0      0      0      0      0      0      0  \n",
       "9989      0      0      0      0      0      0      0      0  \n",
       "9990      0      0      0      0      0      0      0      0  \n",
       "9991      0      0      0      0      0      0      0      0  \n",
       "9992      0      0      0      0      0      0      0      0  \n",
       "9993      0      0      0      0      0      0      0      0  \n",
       "9994      0      0      0      0      0      0      0      0  \n",
       "9995      0      0      0      0      0      0      0      0  \n",
       "9996      0      0      0      0      0      0      0      0  \n",
       "9997      0      0      0      0      0      0      0      0  \n",
       "9998      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[9999 rows x 785 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see it as n x m matrix, with n = 9999 examples(images) in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select 5 random examples from the dataset\n",
    "random_examples = random.sample(range(train.shape[0]), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original image is found by reshaping the rows\n",
    "orig_img = [np.array(train.iloc[element,1:]).reshape(28,28) for element in random_examples]\n",
    "\n",
    "#784 dimensional array form\n",
    "array_form = [np.array(train.iloc[element,1:]) for element in random_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the original images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5)\n",
    "i=0\n",
    "for x in axes:\n",
    "    x.imshow(orig_img[i], cmap = 'gist_gray')\n",
    "    i += 1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing the 784-dimensional array\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5)\n",
    "i=0\n",
    "for x in axes:\n",
    "    x.imshow(array_form[0].reshape(784, 1), aspect = 0.02, cmap='gist_gray')\n",
    "    i += 1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find the general statistical properties of the matrix 'train'. As we know that many of the pixels have intensity '0'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>...</th>\n",
       "      <th>0.658</th>\n",
       "      <th>0.659</th>\n",
       "      <th>0.660</th>\n",
       "      <th>0.661</th>\n",
       "      <th>0.662</th>\n",
       "      <th>0.663</th>\n",
       "      <th>0.664</th>\n",
       "      <th>0.665</th>\n",
       "      <th>0.666</th>\n",
       "      <th>0.667</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "      <td>9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.443144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179318</td>\n",
       "      <td>0.163616</td>\n",
       "      <td>0.052605</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.895897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.674433</td>\n",
       "      <td>5.736359</td>\n",
       "      <td>2.420125</td>\n",
       "      <td>0.060003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 7       0     0.1     0.2     0.3     0.4     0.5     0.6  \\\n",
       "count  9999.000000  9999.0  9999.0  9999.0  9999.0  9999.0  9999.0  9999.0   \n",
       "mean      4.443144     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "std       2.895897     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "min       0.000000     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25%       2.000000     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "50%       4.000000     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "75%       7.000000     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "max       9.000000     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "          0.7     0.8   ...          0.658        0.659        0.660  \\\n",
       "count  9999.0  9999.0   ...    9999.000000  9999.000000  9999.000000   \n",
       "mean      0.0     0.0   ...       0.179318     0.163616     0.052605   \n",
       "std       0.0     0.0   ...       5.674433     5.736359     2.420125   \n",
       "min       0.0     0.0   ...       0.000000     0.000000     0.000000   \n",
       "25%       0.0     0.0   ...       0.000000     0.000000     0.000000   \n",
       "50%       0.0     0.0   ...       0.000000     0.000000     0.000000   \n",
       "75%       0.0     0.0   ...       0.000000     0.000000     0.000000   \n",
       "max       0.0     0.0   ...     253.000000   253.000000   156.000000   \n",
       "\n",
       "             0.661   0.662   0.663   0.664   0.665   0.666   0.667  \n",
       "count  9999.000000  9999.0  9999.0  9999.0  9999.0  9999.0  9999.0  \n",
       "mean      0.000600     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "std       0.060003     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "min       0.000000     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "25%       0.000000     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "50%       0.000000     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "75%       0.000000     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "max       6.000000     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.370'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we find the most likely pixel containing the digit\n",
    "# This can be calculated by finding the pixel column with the maximum mean\n",
    "a = train.describe().loc['mean'].idxmax()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into Training Set and Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test dataset\n",
    "test = pd.read_csv('D:\\Projects\\MLProjects\\mnist_test.csv')\n",
    "\n",
    "# Dividing the train dataset\n",
    "y_train = train.iloc[:,0].values\n",
    "X_train = train.iloc[:,1:].values\n",
    "\n",
    "# Dividing the test dataset\n",
    "y_test = test.iloc[:,0].values\n",
    "X_test = test.iloc[:,1:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be implementing the normal equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the transpose of the Train Data\n",
    "transpose_matrix = X_train.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be followed by the matrix multiplication of the 'transpose_matrix' and 'X_train'\n",
    "matrix_mult = np.matmul(transpose_matrix , X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse of the 'matrix_mult'\n",
    "inverse = np.linalg.pinv(matrix_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Multiplication of the 'transpose_matrix' and 'y_train'\n",
    "mult_ = np.matmul(transpose_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the final weight\n",
    "result_ = np.matmul(inverse,mult_)\n",
    "result_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 784)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking shape of 'X_test'\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making final predictions on the 'X-test' data\n",
    "y_pred = np.dot(X_test, result_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.60094179,  6.66979254,  1.7626087 , ..., -1.56534493,\n",
       "        9.0202329 ,  1.80550508])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_test\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Implementation\n",
    "We shall be implementing gradient descent with the cost fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizes each pixel column[-1,1]\n",
    "def normalize(X):\n",
    "    M = X.shape[1]\n",
    "    for i in range(M):\n",
    "        if np.any(X[:,i]) != 0:\n",
    "            min_value = X[:,i].min()\n",
    "            max_value = X[:,i].max()\n",
    "            X[:,i] =(2*X[:,i]- min_value - max_value)/(max_value - min_value)\n",
    "\n",
    "def append_ones(X):\n",
    "    s = X.shape[0]\n",
    "    ones = np.ones(shape=(s,1))\n",
    "    return np.concatenate((ones, X), axis=1)\n",
    "\n",
    "# Hence using the normalize and append_ones on both 'X_train' and 'X_test'\n",
    "X_train = np.array(X_train)\n",
    "normalize(X_train)\n",
    "X_train = append_ones(X_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "normalize(X_test)\n",
    "X_test = append_ones(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to create a vector for each digit to be stored\n",
    "y_target = []\n",
    "for i in range(10):\n",
    "    y_target.append(y_train.apply(lambda x: 1 if x == i else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=[]\n",
    "\n",
    "#Cost Function \n",
    "def cost(X, y , theta):\n",
    "    dim = X.shape[0]\n",
    "    dot_product = np.power(((X @ theta.T) - y), 2)\n",
    "    return np.sum(dot_product) / (2 * dim)\n",
    "    \n",
    "#Gradient of the cost function\n",
    "def grad_cost(X, y, theta):\n",
    "    dim = X.shape[0]\n",
    "    pred = np.dot(X,theta)\n",
    "    c1 = 1/dim * np.transpose(pred-y)\n",
    "    return np.transpose(np.dot(c1,X))\n",
    "\n",
    "#Gradient descent \n",
    "def grad_descent(X, y, theta, learning_par, num_iter):\n",
    "    for i in range(num_iter):\n",
    "        theta = theta - learning_par*grad_cost(X,y,theta)\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Completed for this digit!\n",
      "1: Completed for this digit!\n",
      "2: Completed for this digit!\n",
      "3: Completed for this digit!\n",
      "4: Completed for this digit!\n",
      "5: Completed for this digit!\n",
      "6: Completed for this digit!\n",
      "7: Completed for this digit!\n",
      "8: Completed for this digit!\n",
      "9: Completed for this digit!\n"
     ]
    }
   ],
   "source": [
    "# Main Implementation\n",
    "for i in range(10):\n",
    "    y_temp = np.array(y_target[i])\n",
    "    y_temp = y_temp.reshape(y_train.shape[0],1)\n",
    "\n",
    "    theta_temp = np.zeros(shape=(X_train.shape[1],1))\n",
    "\n",
    "    learning_rate = 0.0000001\n",
    "    iterations = 1000\n",
    "\n",
    "    theta_temp = grad_descent(X_train,y_temp,theta_temp,learning_rate,iterations)\n",
    "    theta.append(theta_temp)\n",
    "    print('{}: Completed for this digit!'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7315\n"
     ]
    }
   ],
   "source": [
    "result = [np.dot(X_test,theta[i]) for i in range(10)]\n",
    "result = np.transpose(np.array(result)).reshape(X_test.shape[0],10)\n",
    "\n",
    "prediction = (np.array([element.argmax() for element in result])).reshape(X_test.shape[0],1)\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)\n",
    "\n",
    "\n",
    "accuracy = sum(prediction == y_test)[0]/(y_test.shape[0])\n",
    "print('Accuracy is: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is very good, around 73.15%. We shall now implement Perceptron Learning and see if we can further improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Learning\n",
    "Furthering our learning to see if this increases the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we shall create functions to work on Perceptron Learning Model\n",
    "def create_weights(data):\n",
    "    a, b = np.shape(data)\n",
    "    weights = np.random.rand(b,1)\n",
    "    return weights\n",
    "\n",
    "weights = create_weights(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(points, weights):\n",
    "    b = np.dot(points, weights)\n",
    "    a = b>0\n",
    "    return a*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to call this for all the 10 digits but first we need to make it work for one number\n",
    "def one_digit(labels, number):\n",
    "    return (labels == number)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shall use a function to update values, and call the predict function\n",
    "def update(weights, points, labels, learning_rate=.1):\n",
    "    predicted = predict(points, weights)\n",
    "    temp = np.zeros(np.shape(weights))\n",
    "    temp[:,0] = learning_rate*(labels-predicted)*points\n",
    "    return temp+weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we shall train the perceptron on the training dataset\n",
    "def perceptron(data, labels, weights, learning_rate = .001, iterations = 100):\n",
    "    for j in range(0, iterations):\n",
    "        for i in range(0, len(data)):\n",
    "            weights = update(weights, data[i], labels[i], learning_rate)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling this for all 10 digits\n",
    "def all_digits(data,labels):\n",
    "    c,d = np.shape(data)\n",
    "    w = create_weights(data)\n",
    "    weights = []\n",
    "    for i in range(0,  len(np.unique(labels))):\n",
    "        z = one_digit(labels, i)\n",
    "        a = perceptron(data, z, w, .1, 4)\n",
    "        weights.append(a[:,0])\n",
    "    return np.asarray(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Value = all_digits(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(data, weights):\n",
    "    a = np.dot(data,np.transpose(weights))\n",
    "    b = len(np.shape(data))\n",
    "    if b == 1:\n",
    "        return np.argmax(a)\n",
    "    return np.argmax(a, axis=1)\n",
    "\n",
    "\n",
    "def fit(data, labels, weights):\n",
    "    a = np.shape(labels)\n",
    "    predicted = result(data, weights)\n",
    "    correct = predicted == labels\n",
    "    accuracy = np.sum(correct)/float(a[0])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854\n"
     ]
    }
   ],
   "source": [
    "# Now we shall see the accuracy on the test dataset\n",
    "print(fit(X_test, y_test, Value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an improvement from the gradient descent. The accuracy has increased further from 73.15% to 85.4%. This is a brilliant accuracy of the dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
